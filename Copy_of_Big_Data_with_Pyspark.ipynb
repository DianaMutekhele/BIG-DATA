{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Big Data with Pyspark.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DianaMutekhele/BIG-DATA/blob/main/Copy_of_Big_Data_with_Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vThxt_1u5oW"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "More than 500 million tweets, 90 billion emails, 65 million WhatsApp messages are sent – all in a single day! 4 Petabytes of data are generated only on Facebook in 24 hours. That’s incredible!\n",
        "This, of course, comes with challenges of its own. How does a data science team capture this amount of data? How do you process it and build machine learning models?\n",
        "\n",
        "-This is where Spark comes into the picture. Spark is written in Scala and it provides APIs to work with Scala, JAVA, Python, and R. PySpark is the Python API written in Python to support Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzAJongMwPz9"
      },
      "source": [
        "## Why Pyspark?\n",
        "\n",
        "- One way of handling Big Data is to use a distributed frameworks like Hadoop but these frameworks require a lot of read-write operations on a hard disk which makes it very expensive in terms of time and speed\n",
        "-PySpark deals with this in an efficient and easy-to-understand way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpPkyX4_9GNg"
      },
      "source": [
        "## Important Key Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35EvYDh66ILR"
      },
      "source": [
        "## Partitions in Spark\n",
        "-Partitioning means that the complete data is not present in a single place. It is divided into multiple chunks and these chunks are placed on different nodes.\n",
        "\n",
        "- If you have one partition, Spark will only have a parallelism of one, even if you have thousands of executors. \n",
        "- Also, if you have many partitions but only one executor, Spark will still only have a parallelism of one because there is only one computation resource\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx_2cmmk73hN"
      },
      "source": [
        "## Transformations in Spark\n",
        "- Data structures are immutable in Spark therefore In order to make any change, we need to instruct Spark on how we would like to modify our data.\n",
        "- Narrow Transformation: all the elements that are required to compute the results of a single partition live in the single partition\n",
        "- wide transformation: all the elements that are required to compute the results of single partitions may live in more than one partition \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmUyjj42u9Z7",
        "outputId": "2be19aed-a760-4b73-c518-f373d95f4ea0"
      },
      "source": [
        "'''\n",
        "connecting to drive\n",
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEzl0fha_-f0"
      },
      "source": [
        "##Setting up pyspark\n",
        "Spark is written in the Scala programming language and requires the Java Virtual Machine (JVM) to run. Therefore we download java first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9PS_gt-_Qq8"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r9CECSwASIt"
      },
      "source": [
        "#install apache(http://spark.apache.org/downloads.html)\n",
        "\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DGPdFgbArbD",
        "outputId": "5391892a-a792-4ad2-cf7b-b0c1e3864754"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive\t     spark-3.1.1-bin-hadoop2.7.tgz\n",
            "sample_data  spark-3.1.1-bin-hadoop2.7.tgz.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSojFgWrAeHU"
      },
      "source": [
        "#unzip the folder\n",
        "!tar xf spark-3.1.1-bin-hadoop2.7.tgz\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJyMuVylAmBk"
      },
      "source": [
        "\"\"\"\n",
        "we need to install findspark to help as locate the disk and import it as a single library\n",
        "https://pypi.org/project/findspark/\n",
        "\"\"\"\n",
        "!pip install -q findspark\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1JqGIgJCEwl"
      },
      "source": [
        "## Set up Environment Variable so that we are able to run pyspark on Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSswPtmyB68Z"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsL0v485CRis"
      },
      "source": [
        "#we need to locate spark,therefore we import it and use its init()\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kKOJfX_4CpkF",
        "outputId": "41a16ff2-8af8-4f5c-e252-65b3809bdf2f"
      },
      "source": [
        "findspark.find()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/spark-3.1.1-bin-hadoop2.7'"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UITF6vGeD4kz"
      },
      "source": [
        "Now, we can import SparkSession from pyspark.sql and create a SparkSession, which is the entry point to Spark.\n",
        "\n",
        "You can give a name to the session using appName() and add some configurations with config() if you wish.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cJ1yU-NCyrA"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "5r_ddfNIIJxT",
        "outputId": "de9dd532-fdd6-4979-932c-bafce275dda2"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://22ef4fcf3d13:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7faf11957390>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKP-oPh7Jr-y"
      },
      "source": [
        "## If you want to view UI,we need to create a public URL\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOIZu_leEnOK",
        "outputId": "cc947816-480d-487d-ba21-70e2dc32c128"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-29 10:21:24--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.237.133.81, 18.205.222.128, 54.161.241.46, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.237.133.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  8.22MB/s    in 1.6s    \n",
            "\n",
            "2021-09-29 10:21:26 (8.22 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13832437/13832437]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s405DyvqJTKz",
        "outputId": "d1e8ff32-9815-4f44-a177-d0e298c9ed49"
      },
      "source": [
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://afc6-35-190-156-169.ngrok.io\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlULh2Cvi-EY"
      },
      "source": [
        "'''\n",
        "Load data into pyspark\n",
        "'''\n",
        "df =spark.read.csv(\"/content/drive/MyDrive/blood_pressure.csv\",header=True,inferSchema=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLn0km33kMT4"
      },
      "source": [
        "Exploratory Data Analysis(EDA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlSUV2_7jR29",
        "outputId": "bd8728d2-051c-466d-a4b9-438556d71eb2"
      },
      "source": [
        "'''\n",
        "show column details\n",
        "df.info() > pandas equivalent\n",
        "'''\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- patient: integer (nullable = true)\n",
            " |-- sex: string (nullable = true)\n",
            " |-- agegrp: string (nullable = true)\n",
            " |-- bp_before: integer (nullable = true)\n",
            " |-- bp_after: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rq9RVhvnka_2",
        "outputId": "d12c9cf0-67d5-49c4-9915-332cd0d5e375"
      },
      "source": [
        "'''\n",
        "displaying a given number of rows\n",
        "df.head()#pandas equivalent\n",
        "'''\n",
        "df.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+------+---------+--------+\n",
            "|patient| sex|agegrp|bp_before|bp_after|\n",
            "+-------+----+------+---------+--------+\n",
            "|      1|Male| 30-45|      143|     153|\n",
            "|      2|Male| 30-45|      163|     170|\n",
            "|      3|Male| 30-45|      153|     168|\n",
            "|      4|Male| 30-45|      153|     142|\n",
            "|      5|Male| 30-45|      146|     141|\n",
            "+-------+----+------+---------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c96Q13GplCML",
        "outputId": "7e727970-6a34-4916-9664-e10ead8fcc8f"
      },
      "source": [
        "'''\n",
        "Number of rows in df\n",
        "df.shape[0] >> pandas equivalent\n",
        "'''\n",
        "df.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GRvksZ6l4bm",
        "outputId": "cc5f164a-3d3d-4c88-e407-879cfaeb4da4"
      },
      "source": [
        "'''\n",
        "Display Specific columns\n",
        "'''\n",
        "df.select('bp_before','bp_after').show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+\n",
            "|bp_before|bp_after|\n",
            "+---------+--------+\n",
            "|      143|     153|\n",
            "|      163|     170|\n",
            "|      153|     168|\n",
            "|      153|     142|\n",
            "|      146|     141|\n",
            "+---------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_fkyx0Yl8_-"
      },
      "source": [
        "x_df=df.select(\"bp_before\",'bp_after','agegrp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB9mrRjRmuKq",
        "outputId": "4de5d758-ae8a-4e69-c98d-d6275957acff"
      },
      "source": [
        "x_df.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+------+\n",
            "|bp_before|bp_after|agegrp|\n",
            "+---------+--------+------+\n",
            "|      143|     153| 30-45|\n",
            "|      163|     170| 30-45|\n",
            "|      153|     168| 30-45|\n",
            "|      153|     142| 30-45|\n",
            "|      146|     141| 30-45|\n",
            "+---------+--------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so4ZrczxmzcY",
        "outputId": "d0f3c3aa-cc58-4b48-ec43-60d045da46cb"
      },
      "source": [
        "#statistical summary\n",
        "df.describe().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+------+------+-----------------+------------------+\n",
            "|summary|           patient|   sex|agegrp|        bp_before|          bp_after|\n",
            "+-------+------------------+------+------+-----------------+------------------+\n",
            "|  count|               120|   120|   120|              120|               120|\n",
            "|   mean|              60.5|  null|  null|           156.45|151.35833333333332|\n",
            "| stddev|34.785054261852174|  null|  null|11.38984510116671|14.177622226198425|\n",
            "|    min|                 1|Female| 30-45|              138|               125|\n",
            "|    max|               120|  Male|   60+|              185|               185|\n",
            "+-------+------------------+------+------+-----------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73FPjGZBm_YC",
        "outputId": "d07ba2d7-a813-4d6a-e12a-d8b667df261b"
      },
      "source": [
        "'''\n",
        "get specific values within a given column\n",
        "'''\n",
        "df.select(\"sex\").distinct().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|   sex|\n",
            "+------+\n",
            "|Female|\n",
            "|  Male|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufAPUmVXnt-f",
        "outputId": "f6fc971a-7e7a-4631-e45b-356a11e07313"
      },
      "source": [
        "df.select(\"agegrp\").distinct().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|agegrp|\n",
            "+------+\n",
            "|   60+|\n",
            "| 30-45|\n",
            "| 46-59|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXxSuRhuoVKX"
      },
      "source": [
        "'''\n",
        "fill missing values\n",
        "'''\n",
        "df=df.fillna({\"bp_before\":123,\"bp_before\":1})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PFPcZ5oor7W"
      },
      "source": [
        "'''\n",
        "save a file\n",
        "'''\n",
        "df.write.csv(\"/content/results.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kI5feVzym9k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0bbb220-9e42-4dce-e91b-5d7b4dced1b2"
      },
      "source": [
        "'''check the number of partitions available\n",
        "'''\n",
        "df.rdd.getNumPartitions()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}